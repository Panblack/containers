
## nodes
``
192.168.1.120  k8s-node-template
192.168.1.121  k8s-node001.lab.example.com k8s-node001
192.168.1.122  k8s-node002.lab.example.com k8s-node002
192.168.1.123  k8s-node003.lab.example.com k8s-node003
#192.168.1.124  k8s-node004.lab.example.com k8s-node004
192.168.1.125  k8s-node005.lab.example.com k8s-node005

```

## Config template VM
```
#sudo
##ubuntu:
echo "ubuntu  ALL=(ALL) NOPASSWD:ALL" | sudo tee -a /etc/sudoers.d/ubuntu

#sysctl
cat << eof | sudo tee /etc/sysctl.d/20-k8s.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
vm.swappiness = 0
eof

sudo sysctl --system

#kernel modules
##ubuntu:
echo 'br_netfilter' | sudo tee -a /etc/modules 

sudo modprobe overlay
sudo modprobe br_netfilter
	$ lsmod |grep -E 'overlay|netfilter'
	br_netfilter           24576  0
	bridge                278528  1 br_netfilter
	overlay               139264  0


#swapoff
sudo swapoff -a
sudo cp -p /etc/fstab /etc/fstab.bak

##ubuntu:
sudo sed -ir 's/.*swap/#&/g' /etc/fstab
rm -rf /swap.img
free -th


#ubuntu: turn off autoupgrade
sudo sed -i 's/1/0/g' /etc/apt/apt.conf.d/20auto-upgrades
sudo sed -i 's/1/0/g' /etc/apt/apt.conf.d/10periodic
sudo systemctl disable unattended-upgrades.service


##ubuntu:
###install containerd
sudo apt-get update; sudo apt-get upgrade -y
sudo apt-get install ca-certificates curl gnupg lsb-release tree sysstat -y
sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt-get update
sudo apt-get install containerd.io


#aliyun k8s repo & install kube*
curl -s https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add -
echo "deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main" >>/etc/apt/sources.list.d/kubernetes.list
sudo apt-get update
sudo apt-get install kubelet=1.24.1-00 kubectl=1.24.1-00 kubeadm=1.24.1-00 -y
echo -e "source <(kubeadm completion bash)\nsource <(kubectl completion bash)\nsource <(crictl completion bash)" >> ~/.profile


#containerd
sudo mkdir /etc/containerd
sudo cp -p /etc/containerd/config.toml /etc/containerd/config.toml.bak
sudo containerd config default > containerd.default.config.toml
sudo cp -p /etc/containerd/config.toml containerd.config.toml.rpm
sudo cp containerd.default.config.toml /etc/containerd/config.toml
sudo vim /etc/containerd/config.toml

	##ubuntu:
	$ sudo diff /etc/containerd/config.toml containerd.default.config.toml
	56c56
	<     sandbox_image = "registry.aliyuncs.com/google_containers/pause:3.7"
	---
	>     sandbox_image = "k8s.gcr.io/pause:3.5"
	112c112
	<             SystemdCgroup = true
	---
	>             SystemdCgroup = false
	138,141d137
	<        [plugins."io.containerd.grpc.v1.cri".registry.mirrors."docker.io"]
       #<          endpoint = ["https://docker.mirrors.ustc.edu.cn"]
	<          endpoint = ["https://<your_accelerator>.mirror.aliyuncs.com"]
	<        [plugins."io.containerd.grpc.v1.cri".registry.mirrors."k8s.gcr.io"]
	<           endpoint = ["https://registry.aliyuncs.com/google_containers"]
	<        [plugins."io.containerd.grpc.v1.cri".registry.mirrors."registry.k8s.io"]
	<           endpoint = ["https://registry.aliyuncs.com/google_containers"]
        <        [plugins."io.containerd.grpc.v1.cri".registry.mirrors."quay.io"]
        <          endpoint = ["https://quay.azk8s.cn"]


#configure crictl
sudo crictl config --set runtime-endpoint=unix://var/run/containerd/containerd.sock --set image-endpoint=unix://var/run/containerd/containerd.sock
	$ cat /etc/crictl.yaml
	runtime-endpoint: "unix://var/run/containerd/containerd.sock"
	image-endpoint: "unix://var/run/containerd/containerd.sock"
	timeout: 0
	debug: false
	pull-image-on-create: false
	disable-pull-on-run: false

sudo reboot
```


## Clone VMs
```
##ubuntu:
sudo hostnamectl set-hostname k8s-node001.lab.example.com
sudo init 6
sudo hostnamectl set-hostname k8s-node002.lab.example.com
sudo sed -i 's/120/122/g' /etc/netplan/00-installer-config.yaml
sudo init 6
sudo hostnamectl set-hostname k8s-node003.lab.example.com
sudo sed -i 's/120/123/g' /etc/netplan/00-installer-config.yaml
sudo init 6
sudo hostnamectl set-hostname k8s-node004.lab.example.com
sudo sed -i 's/120/124/g' /etc/netplan/00-installer-config.yaml
sudo init 6

sudo cat /sys/class/dmi/id/product_uuid
ip a s ens3

```

## Install cluster
```
sudo kubeadm config images pull --image-repository=registry.aliyuncs.com/google_containers

	$ sudo kubeadm config images pull --image-repository=registry.aliyuncs.com/google_containers
	[config/images] Pulled registry.aliyuncs.com/google_containers/kube-apiserver:v1.24.2
	[config/images] Pulled registry.aliyuncs.com/google_containers/kube-controller-manager:v1.24.2
	[config/images] Pulled registry.aliyuncs.com/google_containers/kube-scheduler:v1.24.2
	[config/images] Pulled registry.aliyuncs.com/google_containers/kube-proxy:v1.24.2
	[config/images] Pulled registry.aliyuncs.com/google_containers/pause:3.7
	[config/images] Pulled registry.aliyuncs.com/google_containers/etcd:3.5.3-0
	[config/images] Pulled registry.aliyuncs.com/google_containers/coredns:v1.8.6

sudo kubeadm init --kubernetes-version=v1.24.2 --image-repository=registry.aliyuncs.com/google_containers

	$ sudo kubeadm init --kubernetes-version=v1.24.2 --image-repository=registry.aliyuncs.com/google_containers
	[init] Using Kubernetes version: v1.24.2
	[preflight] Running pre-flight checks
	[preflight] Pulling images required for setting up a Kubernetes cluster
	[preflight] This might take a minute or two, depending on the speed of your internet connection
	[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
	[certs] Using certificateDir folder "/etc/kubernetes/pki"
	[certs] Generating "ca" certificate and key
	[certs] Generating "apiserver" certificate and key
	[certs] apiserver serving cert is signed for DNS names [k8s-rocky001.lab.example.com kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.1.131]
	[certs] Generating "apiserver-kubelet-client" certificate and key
	[certs] Generating "front-proxy-ca" certificate and key
	[certs] Generating "front-proxy-client" certificate and key
	[certs] Generating "etcd/ca" certificate and key
	[certs] Generating "etcd/server" certificate and key
	[certs] etcd/server serving cert is signed for DNS names [k8s-rocky001.lab.example.com localhost] and IPs [192.168.1.131 127.0.0.1 ::1]
	[certs] Generating "etcd/peer" certificate and key
	[certs] etcd/peer serving cert is signed for DNS names [k8s-rocky001.lab.example.com localhost] and IPs [192.168.1.131 127.0.0.1 ::1]
	[certs] Generating "etcd/healthcheck-client" certificate and key
	[certs] Generating "apiserver-etcd-client" certificate and key
	[certs] Generating "sa" key and public key
	[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
	[kubeconfig] Writing "admin.conf" kubeconfig file
	[kubeconfig] Writing "kubelet.conf" kubeconfig file
	[kubeconfig] Writing "controller-manager.conf" kubeconfig file
	[kubeconfig] Writing "scheduler.conf" kubeconfig file
	[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
	[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
	[kubelet-start] Starting the kubelet
	[control-plane] Using manifest folder "/etc/kubernetes/manifests"
	[control-plane] Creating static Pod manifest for "kube-apiserver"
	[control-plane] Creating static Pod manifest for "kube-controller-manager"
	[control-plane] Creating static Pod manifest for "kube-scheduler"
	[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
	[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
	[apiclient] All control plane components are healthy after 9.504531 seconds
	[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
	[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
	[upload-certs] Skipping phase. Please see --upload-certs
	[mark-control-plane] Marking the node k8s-rocky001.lab.example.com as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
	[mark-control-plane] Marking the node k8s-rocky001.lab.example.com as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule node-role.kubernetes.io/control-plane:NoSchedule]
	[bootstrap-token] Using token: 0cmooj.unu290wgetsxdzmk
	[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
	[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
	[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
	[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
	[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
	[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
	[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
	[addons] Applied essential addon: CoreDNS
	[addons] Applied essential addon: kube-proxy
	
	Your Kubernetes control-plane has initialized successfully!
	
	To start using your cluster, you need to run the following as a regular user:
	
	  mkdir -p $HOME/.kube
	  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
	  sudo chown $(id -u):$(id -g) $HOME/.kube/config
	
	Alternatively, if you are the root user, you can run:
	
	  export KUBECONFIG=/etc/kubernetes/admin.conf
	
	You should now deploy a pod network to the cluster.
	Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
	  https://kubernetes.io/docs/concepts/cluster-administration/addons/
	
	Then you can join any number of worker nodes by running the following on each as root:
	
	kubeadm join 192.168.1.131:6443 --token 0cmooj.unu290wgetsxdzmk \
	        --discovery-token-ca-cert-hash sha256:e147d7069e36ac8e2fbad441d26da352867f7a6ddef4bb9da28586da77826ae5


```


## Add nodes
```
sudo kubeadm join 192.168.1.131:6443 --token 0cmooj.unu290wgetsxdzmk \
                --discovery-token-ca-cert-hash sha256:e147d7069e36ac8e2fbad441d26da352867f7a6ddef4bb9da28586da77826ae5
```


## cni(calico)
```
wget  https://docs.projectcalico.org/manifests/calico.yaml
grep image.*calico calico.yaml
          image: docker.io/calico/cni:v3.23.1
          image: docker.io/calico/cni:v3.23.1
          image: docker.io/calico/node:v3.23.1
          image: docker.io/calico/kube-controllers:v3.23.1
kubectl apply -f calico.yaml

```
